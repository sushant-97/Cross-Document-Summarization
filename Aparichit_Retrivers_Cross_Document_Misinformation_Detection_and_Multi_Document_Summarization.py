# -*- coding: utf-8 -*-
"""Aparichit_Retrivers_Multi_Document_Summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nk8eKAb8os5W38IfcHIF95asVmi4jgcE
"""

# TEAM APARICHIT_RETRIVERS
# Ashish Giri - 224156005
# Raju Sharma - 224156019
# Sushant Pargaonkar - 224156020

# Dependencies
# !pip install  dgl -f https://data.dgl.ai/wheels/cu118/repo.html
# !pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html
# !pip install transformers
# pytorch

!gdown 1YwgqNi8yQMDuXSEvlB8-TT8o92X1vGQk

!tar -xzvf "/content/data.tar.gz" -C "/content"     #[run this cell to extract tar.gz files]

!git clone https://github.com/shirley-wu/cross-doc-misinfo-detection.git

!pip install -r /content/cross-doc-misinfo-detection/requirements.txt

!pip install  dgl -f https://data.dgl.ai/wheels/cu118/repo.html
!pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html

!pip install transformers

!python /content/cross-doc-misinfo-detection/train.py /content/Crisis /content/Output --accum-step 16 --lr 5e-05 --num-epochs 15 --warmup 100  \
--grad-clip 1.0 --scheduler linear-warmup --model hetero --event-detection-lambda 1.0

from google.colab import drive
drive.mount('/content/drive')

!python /content/cross-doc-misinfo-detection/train.py /content/Crisis /content/Output --accum-step 16 --lr 5e-05 --num-epochs 15 --warmup 100  \
--grad-clip 1.0 --scheduler linear-warmup --model hetero --event-detection-lambda 0.0

!python /content/cross-doc-misinfo-detection/train.py /content/Crisis /content/Output --accum-step 16 --lr 5e-05 --num-epochs 15 --warmup 100  \
--grad-clip 1.0 --scheduler linear-warmup --model fuse --event-detection-lambda 0.0 \
  --feature-ckpt /content/Output/checkpoint-best.event.pt

#Evaluate document-level detector:
!python /content/cross-doc-misinfo-detection/eval.py /content/Crisis /content/drive/MyDrive/project/checkpoint-best.doc.pt --model hetero --set test

!python /content/cross-doc-misinfo-detection/eval.py /content/Crisis /content/Output/checkpoint-best.doc.pt --model fuse --feature-ckpt /content/Output/checkpoint-best.event.pt --set test

!python /content/cross-doc-misinfo-detection/eval.py /content/Crisis /content/Output/checkpoint-best.doc.pt  --model hetero --set valid --dump-best-th

!python /content/cross-doc-misinfo-detection/eval.py /content/Crisis /content/Output/checkpoint-best.doc.pt --model hetero --set test \
  --use-th /content/Output/checkpoint-best.doc.pt.valid.best-th.pkl

def read_doc_text(tree):
    doc_tokens = []
    for seg in tree.find('DOC').find('TEXT').findall('SEG'):
        doc_tokens.append(seg.find('ORIGINAL_TEXT').text)
    return ' '.join(doc_tokens)

from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

!python /content/cross-doc-misinfo-detection/eval.py /content/Crisis /content/checkpoint-best.doc.pt --model hetero --set test

import json
import lxml.etree as ET
# Opening JSON file
doc1=[]
with open('/content/data1.json', 'a') as outfile:
  with open("/content/Crisis/split.test.json") as f:
    for line in f:
      cluster={}
      print(line)
      line = json.loads(line)
      cluster["cluster_id"]="cluster_{}".format(line["cluster_id"])
      doc=[]
      count=1;
      totaldoc=""
      if(line["cluster_id"]<10):
        for i in line["documents"]:
          if(i[1]==0 and count<3):
            trees=ET.parse("/content/Crisis/ltf/{}.ltf.xml".format(i[0]))
            totaldoc+=read_doc_text(trees)
            totaldoc+=" "
            doc.append(i[0])
            count+=1;
          if(count==5):
            count=0;
            break;
        # doc1.append(totaldoc)
        try:
          if(len(totaldoc.split())>100):
            with open("/content/data/cluster_{}".format(line["cluster_id"]), 'w') as file:
              summ=summarizer(totaldoc,min_length=142)
              file.write(summ[0]['summary_text'])
          cluster["doc"]=doc
          json.dump(cluster, outfile)
          outfile.write('\n')
        except(e):
          print(e)
          pass

import json
doc=[]
listoffile=os.listdir("/content/datasum/")
with open('/content/data.json', 'w') as outfile:
  with open("/content/Crisis/split.train.json") as f:
    for line in f:
      cluster={}
      line = json.loads(line)
      id="cluster_{}".format(line["cluster_id"])
      if(id in listoffile and line["cluster_id"]!=423 ):
        cluster["cluster_id"]=line["cluster_id"]
        doc=[]
        for i in line["documents"]:
            doc.append(i)
        cluster["documents"]=doc
        json.dump(cluster, outfile)
        outfile.write('\n')

import shutil

# Folder path to be zipped
folder_path = "/content/data"  # Replace with the path of your folder

# Zip file path to be saved
zip_file_path = "/content/"  # Replace with the desired path and name for the ZIP file

# Create a ZIP archive of the folder
shutil.make_archive(zip_file_path, 'zip', folder_path)

# Download the ZIP file
from google.colab import files
files.download(zip_file_path)

"""#LLM model training"""

# TEAM APARICHIT_RETRIVERS
# Ashish Giri - 224156005
# Raju Sharma - 224156019
# Sushant Pargaonkar - 224156020

!gdown 1BtJA5UZMvas0G35SjBmac8vU9R5ciRUl

!unzip  /content/emb.zip -d /content/embedding/

!unzip /content/my_folder.zip.zip -d /content/datasum/

print(len(doc_embedding.shape))

import nltk
import json
import lxml.etree as ET
nltk.download('punkt')
from nltk.corpus import stopwords
import string
punctuation = set(string.punctuation)
nltk.download('stopwords')
# Tokenize the documents
alldocs=[]
stop_words = set(stopwords.words('english'))
def tokenize_documents():
    tokens = []
    with open("/content/data.json") as f:
      for line in f:
        line = json.loads(line)
        totaldoc=""
        for i in line["documents"]:
          if(i[1]==0):
            trees=ET.parse("/content/Crisis/ltf/{}.ltf.xml".format(i[0]))
            totaldoc+=read_doc_text(trees)
        alldocs.append(totaldoc)
        document_tokens = nltk.word_tokenize(totaldoc)
        tokens.extend([token.lower() for token in document_tokens
                       if token.lower() not in stop_words and token.lower() not in punctuation and not token.isdigit()])
    return tokens

tokens = tokenize_documents()
print(len(tokens))

"""#create vocab code"""

from collections import Counter

# Build the vocabulary
def build_vocabulary(tokens, max_vocab_size=None):
    counter = Counter(tokens)
    if max_vocab_size is not None:
        counter = counter.most_common(max_vocab_size)
    vocabulary = {token: i for i, (token, _) in enumerate(counter)}
    return vocabulary

# Build the vocabulary
def build_vocabulary1(tokens, max_vocab_size=None):
    counter = Counter(tokens)
    if max_vocab_size is not None:
        counter = counter.most_common(max_vocab_size)
    vocabulary1 = {i: token for i, (token, _) in enumerate(counter)}
    return vocabulary1

vocabulary = build_vocabulary(tokens, max_vocab_size=10000)
vocabulary1 = build_vocabulary(tokens, max_vocab_size=10000)

import json

# Save the vocabulary to a file
def save_vocabulary(vocabulary, filepath):
    with open(filepath, 'w') as file:
        json.dump(vocabulary, file)

vocab_filepath = 'vocabulary.json'
save_vocabulary(vocabulary, vocab_filepath)

print(vocabulary[1])

def create_labels(summary_documents, vocabulary):
    labels = []
    for summary in summary_documents:
        summary_tokens = nltk.word_tokenize(summary.lower())
        
        label = [vocabulary[token] for token in summary_tokens if token in vocabulary]
        labels.append(label)
    return labels

summary_documents=[]
for i in os.listdir("/content/datasum/"):
  if(i!=".ipynb_checkpoints"):
    with open("/content/datasum/"+i,'r') as file:
      print(i)
      temp=file.readlines()[0]
      summary_documents.append(temp)
labels = create_labels(summary_documents, vocabulary)
print(labels[0])

"""#model training

"""

import os
import torch
inputs=[]
for i in os.listdir("/content/embedding/content/embedding"):
  if(i!=".ipynb_checkpoints"):
    print(i)
    f=torch.load("/content/embedding/content/embedding/"+i)
    inputs.append(f)

from transformers import LEDForConditionalGeneration, LEDTokenizer, LEDConfig
import torch

model_name = "allenai/led-base-16384"
model = LEDForConditionalGeneration.from_pretrained(model_name)
tokenizer = LEDTokenizer.from_pretrained(model_name)

doc_embedding=[]
for i in alldocs:
  
  input_encoding = tokenizer(i, max_length=10, return_tensors="pt",padding=True, truncation=True)
  # print(input_encoding)
  doc_embedding.append(model.get_input_embeddings()(input_encoding["input_ids"]))

print(doc_embedding[0].shape)

trainX=[]
trainY=[]

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
for batch in range(len(inputs)):
  if(inputs[batch].shape[0]==doc_embedding[batch].shape[1]):
    print(doc_embedding[batch].shape,inputs[batch].shape)
    temp3=doc_embedding[batch].reshape(doc_embedding[batch].shape[1],doc_embedding[batch].shape[2]).to(device)
    trainX.append(torch.mean(torch.stack([temp3, inputs[batch]], dim=0), dim=0).detach())
    trainY.append(labels[batch])

print(len(trainY[0]))

optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

model.train()

for epoch in range(100):
    total_loss = 0
    print(epoch)
    for batch in range(len(trainX)):
        temp1=trainX[batch].to(device).reshape((1,trainX[batch].shape[0],trainX[batch].shape[1]))
        temp2=torch.tensor((trainY[batch],))
        temp2=temp2.to(device)
        optimizer.zero_grad()
        outputs = model(
            inputs_embeds   = temp1,
            labels=temp2
        )

        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

    average_loss = total_loss / len(inputs)
    print(f"Epoch {epoch+1} - Average Loss: {average_loss:.4f}")

output_dir = "fine_tuned_led_model"
model.save_pretrained(output_dir)

model_name = "/content/drive/MyDrive/fine_tuned_led_model"
model = LEDForConditionalGeneration.from_pretrained(model_name)

# input_text = alldocs[0] 
# input_encoding = tokenizer(input_text, max_length=10, return_tensors="pt",padding=True, truncation=True)
# # print(input_encoding)
# inputs_embeds = model.get_input_embeddings()(input_encoding["input_ids"])
# print(inputs_embeds.shape)



input_encoding = trainX[3].to(model.device)
input_encoding=input_encoding.reshape((1,input_encoding.shape[0],input_encoding.shape[1]))
print(input_encoding.shape)
with torch.no_grad():
    output = model.generate(
        inputs_embeds  = input_encoding,
        max_length=100,   # Set the desired maximum length of the generated summary
        num_beams=10,     # Set the number of beams for beam search
        early_stopping=True
    )

summary = output[0]
print(summary)

import numpy as np
#Open the JSON file and load its contents
with open("/content/vocabulary.json", "r") as file:
    data = json.load(file)

def convert_labels_to_word(labels, vocab):
  labels=np.asarray(labels)
  words = [vocab[str(label)] for label in labels]
  return ' '.join(words)
print(convert_labels_to_word(summary,data))



a